{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import train_test_split\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch import optim\n",
    "import torch.nn.functional as F\n",
    "from torchvision import datasets, transforms, models\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from sklearn.preprocessing import StandardScaler\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "def set_seed(seed):\n",
    "    random.seed(seed)  # Set the seed for Python's random module\n",
    "    np.random.seed(seed)  # Set the seed for NumPy\n",
    "    torch.manual_seed(seed)  # Set the seed for PyTorch\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.manual_seed(seed)  # Set the seed for CUDA\n",
    "        torch.cuda.manual_seed_all(seed)  # For multi-GPU setups\n",
    "        # Enable deterministic behavior\n",
    "        torch.backends.cudnn.deterministic = True\n",
    "        torch.backends.cudnn.benchmark = False\n",
    "\n",
    "set_seed(4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Balanced shapes of modalities:\n",
      "(1592, 19)\n",
      "(1592, 500)\n",
      "(1592, 500)\n",
      "(1592, 500)\n",
      "(1592, 500)\n",
      "(1592, 800)\n",
      "Labels: (1592,)\n",
      "\n",
      "Class distribution after balancing:\n",
      "Positive samples: 796\n",
      "Negative samples: 796\n"
     ]
    }
   ],
   "source": [
    "# load 6 modalites data \n",
    "from sklearn.utils import resample\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "def load_data():\n",
    "    cln_data = pd.read_csv('data/raw_features_cln.csv')\n",
    "    cnv_data = pd.read_csv('data/raw_features_cnv.csv')\n",
    "    dna_data = pd.read_csv('data/raw_features_dna.csv')\n",
    "    mir_data = pd.read_csv('data/raw_features_mir.csv')\n",
    "    mrna_data = pd.read_csv('data/raw_features_mrna.csv')\n",
    "    wsi_data = pd.read_csv('data/raw_features_wsi.csv')\n",
    "\n",
    "    # keep data where all modalities are present based on patient id\n",
    "    patient_ids = set(cln_data['submitter_id.samples'])\n",
    "    patient_ids = patient_ids.intersection(set(cnv_data['submitter_id.samples']))\n",
    "    patient_ids = patient_ids.intersection(set(dna_data['submitter_id.samples']))\n",
    "    patient_ids = patient_ids.intersection(set(mir_data['submitter_id.samples']))\n",
    "    patient_ids = patient_ids.intersection(set(mrna_data['submitter_id.samples']))\n",
    "    patient_ids = patient_ids.intersection(set(wsi_data['submitter_id.samples']))\n",
    "\n",
    "    cln_data = cln_data[cln_data['submitter_id.samples'].isin(patient_ids)].reset_index(drop=True)\n",
    "    cnv_data = cnv_data[cnv_data['submitter_id.samples'].isin(patient_ids)].reset_index(drop=True)\n",
    "    dna_data = dna_data[dna_data['submitter_id.samples'].isin(patient_ids)].reset_index(drop=True)\n",
    "    mir_data = mir_data[mir_data['submitter_id.samples'].isin(patient_ids)].reset_index(drop=True)\n",
    "    mrna_data = mrna_data[mrna_data['submitter_id.samples'].isin(patient_ids)].reset_index(drop=True)\n",
    "    wsi_data = wsi_data[wsi_data['submitter_id.samples'].isin(patient_ids)].reset_index(drop=True)\n",
    "\n",
    "    # sort data by patient id\n",
    "    cln_data = cln_data.sort_values('submitter_id.samples').reset_index(drop=True)\n",
    "    cnv_data = cnv_data.sort_values('submitter_id.samples').reset_index(drop=True)\n",
    "    dna_data = dna_data.sort_values('submitter_id.samples').reset_index(drop=True)\n",
    "    mir_data = mir_data.sort_values('submitter_id.samples').reset_index(drop=True)\n",
    "    mrna_data = mrna_data.sort_values('submitter_id.samples').reset_index(drop=True)\n",
    "    wsi_data = wsi_data.sort_values('submitter_id.samples').reset_index(drop=True)\n",
    "\n",
    "    # make sure the labels column match for all modalities\n",
    "    cln_labels = cln_data['label_cln']\n",
    "    cnv_labels = cnv_data['label_cnv']\n",
    "    dna_labels = dna_data['label_dna']\n",
    "    mir_labels = mir_data['label_mir']\n",
    "    mrna_labels = mrna_data['label_mrna']\n",
    "    wsi_labels = wsi_data['label_wsi']\n",
    "\n",
    "    assert np.all(cln_labels == cnv_labels), \"Labels mismatch between clinical and CNV data\"\n",
    "    assert np.all(cln_labels == dna_labels), \"Labels mismatch between clinical and DNA data\"\n",
    "    assert np.all(cln_labels == mir_labels), \"Labels mismatch between clinical and miRNA data\"\n",
    "    assert np.all(cln_labels == mrna_labels), \"Labels mismatch between clinical and mRNA data\"\n",
    "    # assert np.all(cln_labels == wsi_labels), \"Labels mismatch between clinical and WSI data\"\n",
    "\n",
    "    # drop labels column and patient id column\n",
    "    cln_data = cln_data.drop(columns=['label_cln', 'submitter_id.samples'])\n",
    "    cnv_data = cnv_data.drop(columns=['label_cnv', 'submitter_id.samples'])\n",
    "    dna_data = dna_data.drop(columns=['label_dna', 'submitter_id.samples'])\n",
    "    mir_data = mir_data.drop(columns=['label_mir', 'submitter_id.samples'])\n",
    "    mrna_data = mrna_data.drop(columns=['label_mrna', 'submitter_id.samples'])\n",
    "    wsi_data = wsi_data.drop(columns=['label_wsi', 'submitter_id.samples'])\n",
    "\n",
    "    # normalize data\n",
    "    stdscalar = StandardScaler()\n",
    "    cln_data = stdscalar.fit_transform(cln_data)\n",
    "    cnv_data = stdscalar.fit_transform(cnv_data)\n",
    "    dna_data = stdscalar.fit_transform(dna_data)\n",
    "    mir_data = stdscalar.fit_transform(mir_data)\n",
    "    mrna_data = stdscalar.fit_transform(mrna_data)\n",
    "    wsi_data = stdscalar.fit_transform(wsi_data)\n",
    "\n",
    "    return cln_data, cnv_data, dna_data, mir_data, mrna_data, wsi_data, cln_labels\n",
    "\n",
    "\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "def balance_modality_data(features_list, labels, method='SMOTE'):\n",
    "    \"\"\"\n",
    "    Balances the dataset for each modality separately.\n",
    "    \n",
    "    Parameters:\n",
    "    - features_list: List of feature matrices, one for each modality.\n",
    "    - labels: Binary labels (numpy array or Series).\n",
    "    - method: Balancing method ('SMOTE' supported here).\n",
    "    \n",
    "    Returns:\n",
    "    - Separate balanced feature matrices for each modality.\n",
    "    - Balanced labels (same across all modalities).\n",
    "    \"\"\"\n",
    "    smote = SMOTE(random_state=42)\n",
    "    balanced_features_list = []\n",
    "    \n",
    "    for modality_features in features_list:\n",
    "        # Balance the current modality's features and labels\n",
    "        balanced_modality_features, balanced_labels = smote.fit_resample(modality_features, labels)\n",
    "        balanced_features_list.append(balanced_modality_features)\n",
    "    \n",
    "    # Unpack balanced features into separate variables and return them\n",
    "    return (balanced_features_list, balanced_labels)\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    # Load the data\n",
    "    cln_data, cnv_data, dna_data, mir_data, mrna_data, wsi_data, labels = load_data()\n",
    "    \n",
    "    # List of modalities\n",
    "    features_list = [cln_data, cnv_data, dna_data, mir_data, mrna_data, wsi_data]\n",
    "    \n",
    "    # Balance the modalities and unpack them\n",
    "    balanced_data, balanced_labels = balance_modality_data(features_list, labels, method='SMOTE')\n",
    "    \n",
    "    # Print shapes\n",
    "    print(\"\\nBalanced shapes of modalities:\")\n",
    "    for data in balanced_data:\n",
    "        print(data.shape)\n",
    "    print(f\"Labels: {balanced_labels.shape}\")\n",
    "\n",
    "    # Class distribution\n",
    "    print(\"\\nClass distribution after balancing:\")\n",
    "    print(f\"Positive samples: {np.sum(balanced_labels == 1)}\")\n",
    "    print(f\"Negative samples: {np.sum(balanced_labels == 0)}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Define a MultimodalDataset class if not already defined\n",
    "class MultimodalDataset(Dataset):\n",
    "    def __init__(self, *modalities, labels):\n",
    "        self.modalities = modalities\n",
    "        self.labels = labels\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return tuple(modality[idx] for modality in self.modalities) + (self.labels[idx],)\n",
    "\n",
    "\n",
    "def load_and_split_data(modalities, labels, test_size=0.2, random_state=42):\n",
    "    \"\"\"\n",
    "    Splits the data into train and test sets, and balances the test set.\n",
    "    \"\"\"\n",
    "    # Split the data into train and test sets\n",
    "    train_modalities, test_modalities = [], []\n",
    "    for modality in modalities:\n",
    "        train, test = train_test_split(modality, test_size=test_size, random_state=random_state)\n",
    "        train_modalities.append(train)\n",
    "        test_modalities.append(test)\n",
    "\n",
    "    labels_train, labels_test = train_test_split(labels, test_size=test_size, random_state=random_state)\n",
    "\n",
    "    return train_modalities, test_modalities, labels_train, labels_test\n",
    "\n",
    "# Function to convert data to tensors\n",
    "def convert_to_tensors(data):\n",
    "    return [torch.tensor(modality, dtype=torch.float32) for modality in data]\n",
    "\n",
    "# Main code\n",
    "def get_train_test_loader():\n",
    "    # Load the data\n",
    "    cln_data, cnv_data, dna_data, mir_data, mrna_data, wsi_data, labels = load_data()\n",
    "    modalities = [cln_data, cnv_data, dna_data, mir_data, mrna_data, wsi_data]\n",
    "    balanced_data, labels = balance_modality_data(modalities, labels)\n",
    "    modalities = []\n",
    "    for data in balanced_data:\n",
    "        modalities.append(data)\n",
    "    # modalities = [cln_data, cnv_data, dna_data, mir_data, mrna_data, wsi_data]\n",
    "    # Split data\n",
    "    train_modalities, test_modalities, labels_train, labels_test = load_and_split_data(modalities, labels)\n",
    "\n",
    "    # train_modalities, labels_train = balance_modality_data(train_modalities, labels_train)\n",
    "    # test_modalities, labels_test = balance_modality_data(test_modalities, labels_test)\n",
    "\n",
    "    # Convert data to tensors\n",
    "    train_tensors = convert_to_tensors(train_modalities)\n",
    "    test_tensors = convert_to_tensors(test_modalities)\n",
    "    labels_train = torch.tensor(labels_train, dtype=torch.long)\n",
    "    labels_test = torch.tensor(labels_test.to_numpy(), dtype=torch.long)\n",
    "\n",
    "    # Create datasets and dataloaders\n",
    "    train_dataset = MultimodalDataset(*train_tensors, labels=labels_train)\n",
    "    test_dataset = MultimodalDataset(*test_tensors, labels=labels_test)\n",
    "\n",
    "    batch_size = 64\n",
    "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "    test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "    return train_modalities, test_modalities, labels_train, labels_test, train_loader, test_loader\n",
    "\n",
    "# Run the main function\n",
    "train_modalities, test_modalities, labels_train, labels_test,_,_ = get_train_test_loader()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ModalityEncoder(nn.Module):\n",
    "    def __init__(self, input_dim, embed_dim, num_heads, num_layers):\n",
    "        super(ModalityEncoder, self).__init__()\n",
    "        self.embedding = nn.Linear(input_dim, embed_dim)\n",
    "        self.transformer = nn.TransformerEncoder(\n",
    "            nn.TransformerEncoderLayer(d_model=embed_dim, nhead=num_heads),\n",
    "            num_layers=num_layers\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            x: Input tensor of shape (batch_size, seq_len, input_dim) or (batch_size, input_dim).\n",
    "        Returns:\n",
    "            Encoded tensor of shape (seq_len, batch_size, embed_dim).\n",
    "        \"\"\"\n",
    "        # If input is 2D (batch_size, input_dim), add a dummy seq_len dimension\n",
    "        if x.dim() == 2:\n",
    "            x = x.unsqueeze(1)  # Shape becomes (batch_size, 1, input_dim)\n",
    "        \n",
    "        # Embed and transpose for Transformer\n",
    "        x = self.embedding(x)  # Shape: (batch_size, seq_len, embed_dim)\n",
    "        x = x.transpose(0, 1)  # Transformer expects (seq_len, batch_size, embed_dim)\n",
    "        \n",
    "        # Pass through the Transformer\n",
    "        x = self.transformer(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CrossAttentionLayer(nn.Module):\n",
    "    def __init__(self, embed_dim, num_heads):\n",
    "        super(CrossAttentionLayer, self).__init__()\n",
    "        self.multihead_attn = nn.MultiheadAttention(embed_dim=embed_dim, num_heads=num_heads)\n",
    "\n",
    "    def forward(self, query, key, value):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            query: Tensor of shape (seq_len, batch_size, embed_dim).\n",
    "            key, value: Tensors of shape (seq_len, batch_size, embed_dim).\n",
    "        Returns:\n",
    "            Fused representation of shape (seq_len, batch_size, embed_dim).\n",
    "        \"\"\"\n",
    "        attn_output, _ = self.multihead_attn(query, key, value)\n",
    "        return attn_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "from sklearn.metrics import accuracy_score, roc_auc_score, f1_score, precision_score, roc_curve, precision_recall_curve\n",
    "from sklearn.metrics import recall_score, matthews_corrcoef, average_precision_score, confusion_matrix\n",
    "\n",
    "# Function to evaluate the model\n",
    "def test_model(model, test_loader, device=\"cpu\"):\n",
    "    model.eval()  # Set model to evaluation mode\n",
    "    all_preds = []\n",
    "    all_targets = []\n",
    "    all_probs = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch in test_loader:\n",
    "            # Unpack modalities and labels from the batch\n",
    "            modalities = [modality.to(device) for modality in batch[:-1]]\n",
    "            labels = batch[-1].to(device)\n",
    "\n",
    "            # Forward pass\n",
    "            _,outputs = model(modalities)\n",
    "            probs = torch.sigmoid(outputs).squeeze()  # Apply sigmoid for binary classification probabilities\n",
    "\n",
    "            # Predictions (threshold at 0.5)\n",
    "            preds = (probs >= 0.52).long()\n",
    "\n",
    "            # Collect predictions and targets\n",
    "            all_preds.extend(preds.cpu().numpy())\n",
    "            all_probs.extend(probs.cpu().numpy())\n",
    "            all_targets.extend(labels.cpu().numpy())\n",
    "\n",
    "    # Calculate metrics\n",
    "    accuracy = accuracy_score(all_targets, all_preds)\n",
    "    auc = roc_auc_score(all_targets, all_probs)\n",
    "    f1 = f1_score(all_targets, all_preds)\n",
    "    precision = precision_score(all_targets, all_preds)\n",
    "    recall = recall_score(all_targets, all_preds)\n",
    "    mcc = matthews_corrcoef(all_targets, all_preds)\n",
    "    auprc = average_precision_score(all_targets, all_probs)\n",
    "    cm = confusion_matrix(all_targets, all_preds)\n",
    "    specificity = cm[0, 0] / cm[0, :].sum()  \n",
    "\n",
    "    print(\"Test Metrics:\")\n",
    "    print(f\"Accuracy: {accuracy:.4f}\")\n",
    "    print(f\"AUC: {auc:.4f}\")\n",
    "    print(f\"F1 Score: {f1:.4f}\")\n",
    "    print(f\"Precision: {precision:.4f}\")\n",
    "    print(f\"Recall: {recall:.4f}\")\n",
    "    print(f\"Specificity: {specificity:.4f}\")\n",
    "    print(f\"MCC: {mcc:.4f}\")\n",
    "    print(f\"AUPRC: {auprc:.4f}\")\n",
    "\n",
    "    # plot confusion matrix\n",
    "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', cbar=False)\n",
    "    plt.xlabel('Predicted')\n",
    "    plt.ylabel('True')\n",
    "    plt.title('Confusion Matrix')\n",
    "    plt.show()\n",
    "\n",
    "    return {\n",
    "        \"accuracy\": accuracy,\n",
    "        \"auc\": auc,\n",
    "        \"f1\": f1,\n",
    "        \"precision\": precision,\n",
    "        \"recall\": recall,\n",
    "        \"specificity\": specificity,\n",
    "        \"mcc\": mcc,\n",
    "        \"auprc\": auprc\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import torch\n",
    "# import torch.nn as nn\n",
    "# import torch.nn.functional as F\n",
    "# import torch.optim as optim\n",
    "# from tqdm import tqdm\n",
    "\n",
    "\n",
    "# # Multimodal Model\n",
    "# class MultimodalModel(nn.Module):\n",
    "#     def __init__(self, input_dims, embed_dim, num_heads, num_layers):\n",
    "#         super(MultimodalModel, self).__init__()\n",
    "        \n",
    "#         # Modality encoders for each input\n",
    "#         self.modality_encoders = nn.ModuleList([\n",
    "#             ModalityEncoder(input_dim, embed_dim, num_heads, num_layers) for input_dim in input_dims\n",
    "#         ])\n",
    "        \n",
    "#         # Cross-attention layer\n",
    "#         self.cross_attention = CrossAttentionLayer(embed_dim, num_heads)\n",
    "        \n",
    "#         # Final classification head\n",
    "#         self.classification_head = nn.Sequential(\n",
    "#             nn.Linear(embed_dim, embed_dim),\n",
    "#             nn.ReLU(),\n",
    "#             nn.Linear(embed_dim, 1),  # Output 1 value per sample (for binary classification)\n",
    "#             nn.Sigmoid()  # Sigmoid activation for BCE loss\n",
    "#         )\n",
    "\n",
    "\n",
    "#     def forward(self, modalities):\n",
    "#         \"\"\"\n",
    "#         Args:\n",
    "#             modalities: List of tensors, one for each modality.\n",
    "#                         Each tensor should have shape (batch_size, seq_len, input_dim)\n",
    "#                         or (batch_size, input_dim).\n",
    "#         Returns:\n",
    "#             Predictions: Tensor of shape (batch_size, 1).\n",
    "#         \"\"\"\n",
    "#         # Encode each modality\n",
    "#         encoded_modalities = [\n",
    "#             encoder(modality) for encoder, modality in zip(self.modality_encoders, modalities)\n",
    "#         ]\n",
    "\n",
    "#         # Apply cross-attention for each modality as the query\n",
    "#         fused_representations = []\n",
    "#         for i, query in enumerate(encoded_modalities):\n",
    "#             # Use all other modalities as keys and values\n",
    "#             keys_values = torch.cat(\n",
    "#                 [encoded for j, encoded in enumerate(encoded_modalities) if j != i], dim=0\n",
    "#             )\n",
    "#             fused_representation = self.cross_attention(query, keys_values, keys_values)\n",
    "#             # Aggregate across the sequence dimension (mean pooling)\n",
    "#             aggregated_representation = fused_representation.mean(dim=1)\n",
    "#             fused_representations.append(fused_representation)\n",
    "\n",
    "#         # Merge all fused representations (e.g., by averaging)\n",
    "#         final_representation = torch.mean(torch.stack(fused_representations, dim=0), dim=0)\n",
    "\n",
    "#         # Output final classification prediction\n",
    "#         predictions = self.classification_head(final_representation)\n",
    "#         return predictions\n",
    "\n",
    "\n",
    "\n",
    "# # Training function\n",
    "# def train_model(model, train_loader, optimizer, num_epochs, device=\"cpu\"):\n",
    "#     \"\"\"\n",
    "#     Train the multimodal model using Binary Cross-Entropy Loss.\n",
    "#     Args:\n",
    "#         model: The multimodal model.\n",
    "#         train_loader: DataLoader for the training set.\n",
    "#         optimizer: Optimizer for the model.\n",
    "#         num_epochs: Number of training epochs.\n",
    "#         device: Device to train the model on ('cpu' or 'cuda').\n",
    "#     \"\"\"\n",
    "#     model.to(device)\n",
    "#     model.train()\n",
    "\n",
    "#     criterion = nn.BCELoss(reduction='none')  # Compute per-sample loss first\n",
    "\n",
    "\n",
    "#     for epoch in range(num_epochs):\n",
    "#         epoch_loss = 0\n",
    "#         for batch in tqdm(train_loader, desc=f\"Epoch {epoch+1}/{num_epochs}\"):\n",
    "#             # Move batch data to the device\n",
    "#             modalities = [modality.to(device) for modality in batch[:-1]]\n",
    "#             labels = batch[-1].to(device)  # Binary labels\n",
    "\n",
    "#             # Forward pass\n",
    "#             optimizer.zero_grad()\n",
    "#             predictions = model(modalities).squeeze()  # Remove extra dimensions\n",
    "\n",
    "#             # Compute BCE loss\n",
    "#             # Compute BCE loss per sample\n",
    "#             loss = criterion(predictions, labels.float())\n",
    "\n",
    "#             # Apply class weights based on labels\n",
    "#             class_weights = torch.tensor([5.0, 1.0]).to(device)  # Adjust weights for class imbalance\n",
    "#             sample_weights = torch.where(labels == 1, class_weights[1], class_weights[0])\n",
    "#             loss = (loss * sample_weights).mean()  # Apply weights and compute mean loss\n",
    "\n",
    "\n",
    "#             # Backward pass and optimization\n",
    "#             loss.backward()\n",
    "#             optimizer.step()\n",
    "\n",
    "#             epoch_loss += loss.item()\n",
    "\n",
    "#         avg_loss = epoch_loss / len(train_loader)\n",
    "#         print(f\"Epoch {epoch+1}/{num_epochs}, Loss: {avg_loss:.4f}\")\n",
    "#         # test_model(model, train_loader, device=device)\n",
    "#         # test_model(model, test_loader, device=device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Contrastive Loss Function\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from tqdm import tqdm\n",
    "\n",
    "def contrastive_loss(embeddings, labels, margin=1.0):\n",
    "    \"\"\"\n",
    "    Compute contrastive loss for embeddings.\n",
    "    Args:\n",
    "        embeddings: Tensor of shape (batch_size, embed_dim), model embeddings.\n",
    "        labels: Tensor of shape (batch_size), binary labels (0 or 1).\n",
    "        margin: Margin for the contrastive loss.\n",
    "    Returns:\n",
    "        Loss: Scalar tensor representing the contrastive loss.\n",
    "    \"\"\"\n",
    "    # Compute pairwise distances\n",
    "    pairwise_distances = torch.cdist(embeddings, embeddings, p=2)  # Euclidean distances\n",
    "\n",
    "    # Create pairwise label similarity (1 if same class, 0 otherwise)\n",
    "    pairwise_labels = (labels.unsqueeze(1) == labels.unsqueeze(0)).float()\n",
    "\n",
    "    # Positive pairs (same class): minimize distance\n",
    "    positive_loss = pairwise_labels * pairwise_distances.pow(2)\n",
    "\n",
    "    # Negative pairs (different class): enforce margin\n",
    "    negative_loss = (1 - pairwise_labels) * F.relu(margin - pairwise_distances).pow(2)\n",
    "\n",
    "    # Combine losses\n",
    "    loss = (positive_loss + negative_loss).mean()\n",
    "    return loss\n",
    "\n",
    "\n",
    "# Updated Model\n",
    "class MultimodalModel(nn.Module):\n",
    "    def __init__(self, input_dims, embed_dim, num_heads, num_layers):\n",
    "        super(MultimodalModel, self).__init__()\n",
    "        self.modality_encoders = nn.ModuleList([\n",
    "            ModalityEncoder(input_dim, embed_dim, num_heads, num_layers) for input_dim in input_dims\n",
    "        ])\n",
    "        self.cross_attention = CrossAttentionLayer(embed_dim, num_heads)\n",
    "        self.hidden_layer = nn.Linear(embed_dim, embed_dim)  # Hidden layer for embeddings\n",
    "        self.classification_head = nn.Sequential(\n",
    "            nn.Linear(embed_dim, embed_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(embed_dim, 1),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "\n",
    "    def forward(self, modalities):\n",
    "        encoded_modalities = [encoder(modality) for encoder, modality in zip(self.modality_encoders, modalities)]\n",
    "        fused_representations = []\n",
    "        for i, query in enumerate(encoded_modalities):\n",
    "            keys_values = torch.cat(\n",
    "                [encoded for j, encoded in enumerate(encoded_modalities) if j != i], dim=0\n",
    "            )\n",
    "            fused_representation = self.cross_attention(query, keys_values, keys_values)\n",
    "            aggregated_representation = fused_representation.mean(dim=1)\n",
    "            fused_representations.append(fused_representation)\n",
    "\n",
    "        final_representation = torch.mean(torch.stack(fused_representations, dim=0), dim=0)\n",
    "        embeddings = self.hidden_layer(final_representation)  # Extract embeddings\n",
    "        \n",
    "        predictions = self.classification_head(final_representation)\n",
    "        return embeddings, predictions\n",
    "\n",
    "\n",
    "# Updated Training Function\n",
    "def train_model_with_contrastive_loss(model, train_loader, optimizer, num_epochs, device=\"cpu\", margin=1.0, lambda_c=0.5):\n",
    "    \"\"\"\n",
    "    Train the model with contrastive loss and binary cross-entropy loss.\n",
    "    Args:\n",
    "        model: The multimodal model.\n",
    "        train_loader: DataLoader for the training set.\n",
    "        optimizer: Optimizer for the model.\n",
    "        num_epochs: Number of training epochs.\n",
    "        device: Device to train the model on ('cpu' or 'cuda').\n",
    "        margin: Margin for the contrastive loss.\n",
    "        lambda_c: Weight for contrastive loss in the final loss calculation.\n",
    "    \"\"\"\n",
    "    model.to(device)\n",
    "    model.train()\n",
    "\n",
    "    criterion_bce = nn.BCELoss(reduction='none')  # Per-sample BCE loss\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        epoch_loss = 0\n",
    "        for batch in tqdm(train_loader, desc=f\"Epoch {epoch+1}/{num_epochs}\"):\n",
    "            modalities = [modality.to(device) for modality in batch[:-1]]\n",
    "            labels = batch[-1].to(device)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            # Forward pass\n",
    "            embeddings, predictions = model(modalities)\n",
    "            predictions = predictions.squeeze()\n",
    "\n",
    "            # Compute BCE loss\n",
    "            bce_loss = criterion_bce(predictions, labels.float())\n",
    "            class_weights = torch.tensor([5.0, 1.0]).to(device)\n",
    "            sample_weights = torch.where(labels == 1, class_weights[1], class_weights[0])\n",
    "            bce_loss = (bce_loss * sample_weights).mean()\n",
    "\n",
    "            # Compute Contrastive Loss\n",
    "            contrastive_loss_value = contrastive_loss(embeddings, labels, margin)\n",
    "\n",
    "            # Combine losses\n",
    "            total_loss = bce_loss + lambda_c * contrastive_loss_value\n",
    "\n",
    "            # Backward pass and optimization\n",
    "            total_loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            epoch_loss += total_loss.item()\n",
    "\n",
    "        avg_loss = epoch_loss / len(train_loader)\n",
    "        print(f\"Epoch {epoch+1}/{num_epochs}, Loss: {avg_loss:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\User\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\torch\\nn\\modules\\transformer.py:379: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.batch_first was not True(use batch_first for better inference performance)\n",
      "  warnings.warn(\n",
      "Epoch 1/30: 100%|██████████| 20/20 [00:00<00:00, 26.31it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30, Loss: 1.6162\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2/30: 100%|██████████| 20/20 [00:00<00:00, 27.24it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/30, Loss: 1.0570\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3/30: 100%|██████████| 20/20 [00:00<00:00, 27.40it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3/30, Loss: 0.3849\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4/30: 100%|██████████| 20/20 [00:00<00:00, 26.76it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4/30, Loss: 0.2690\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 5/30: 100%|██████████| 20/20 [00:00<00:00, 25.26it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5/30, Loss: 0.1701\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 6/30: 100%|██████████| 20/20 [00:00<00:00, 27.03it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6/30, Loss: 0.1032\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 7/30: 100%|██████████| 20/20 [00:00<00:00, 27.92it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7/30, Loss: 0.0494\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 8/30: 100%|██████████| 20/20 [00:00<00:00, 26.98it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8/30, Loss: 0.0393\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 9/30: 100%|██████████| 20/20 [00:00<00:00, 26.91it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9/30, Loss: 0.0312\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 10/30: 100%|██████████| 20/20 [00:00<00:00, 26.98it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10/30, Loss: 0.0140\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 11/30: 100%|██████████| 20/20 [00:00<00:00, 26.38it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 11/30, Loss: 0.0086\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 12/30: 100%|██████████| 20/20 [00:00<00:00, 27.86it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 12/30, Loss: 0.0064\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 13/30: 100%|██████████| 20/20 [00:00<00:00, 27.42it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 13/30, Loss: 0.0054\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 14/30: 100%|██████████| 20/20 [00:00<00:00, 25.18it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 14/30, Loss: 0.0051\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 15/30: 100%|██████████| 20/20 [00:00<00:00, 26.75it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 15/30, Loss: 0.0040\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 16/30: 100%|██████████| 20/20 [00:00<00:00, 25.79it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 16/30, Loss: 0.0036\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 17/30: 100%|██████████| 20/20 [00:00<00:00, 27.24it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 17/30, Loss: 0.0031\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 18/30: 100%|██████████| 20/20 [00:00<00:00, 26.79it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 18/30, Loss: 0.0028\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 19/30: 100%|██████████| 20/20 [00:00<00:00, 26.00it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 19/30, Loss: 0.0027\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 20/30: 100%|██████████| 20/20 [00:00<00:00, 25.58it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 20/30, Loss: 0.0023\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 21/30: 100%|██████████| 20/20 [00:00<00:00, 25.50it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 21/30, Loss: 0.0022\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 22/30: 100%|██████████| 20/20 [00:00<00:00, 25.98it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 22/30, Loss: 0.0021\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 23/30: 100%|██████████| 20/20 [00:00<00:00, 26.99it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 23/30, Loss: 0.0018\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 24/30: 100%|██████████| 20/20 [00:00<00:00, 24.98it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 24/30, Loss: 0.0017\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 25/30: 100%|██████████| 20/20 [00:00<00:00, 26.01it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 25/30, Loss: 0.0015\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 26/30: 100%|██████████| 20/20 [00:00<00:00, 26.34it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 26/30, Loss: 0.0014\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 27/30: 100%|██████████| 20/20 [00:00<00:00, 26.18it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 27/30, Loss: 0.0013\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 28/30: 100%|██████████| 20/20 [00:00<00:00, 26.90it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 28/30, Loss: 0.0012\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 29/30: 100%|██████████| 20/20 [00:00<00:00, 26.52it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 29/30, Loss: 0.0011\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 30/30: 100%|██████████| 20/20 [00:00<00:00, 26.27it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 30/30, Loss: 0.0011\n",
      "Test Metrics:\n",
      "Accuracy: 1.0000\n",
      "AUC: 1.0000\n",
      "F1 Score: 1.0000\n",
      "Precision: 1.0000\n",
      "Recall: 1.0000\n",
      "Specificity: 1.0000\n",
      "MCC: 1.0000\n",
      "AUPRC: 1.0000\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Example usage\n",
    "def main_training(train=True):\n",
    "    # Load data\n",
    "    _,_,_,_,train_loader, test_loader = get_train_test_loader()  # Assuming your main() function loads data and returns DataLoaders\n",
    "\n",
    "    # Model parameters\n",
    "    input_dims = [19, 500, 500, 500, 500, 800]  # Number of features for each modality\n",
    "    embed_dim = 64  # Common embedding dimension\n",
    "    num_heads = 4  # Number of attention heads\n",
    "    num_layers = 1  # Number of layers in the transformer encoders\n",
    "\n",
    "    # Initialize model, optimizer\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    model = MultimodalModel(input_dims, embed_dim, num_heads, num_layers)\n",
    "    optimizer = optim.Adam(model.parameters(), lr=1e-3)\n",
    "\n",
    "    # Train the model\n",
    "    num_epochs = 30\n",
    "    if train:\n",
    "        train_model_with_contrastive_loss(model, train_loader, optimizer, num_epochs, device)\n",
    "        metrics = test_model(model, train_loader, device)\n",
    "\n",
    "    return model\n",
    "\n",
    "model = main_training()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Metrics:\n",
      "Accuracy: 0.8401\n",
      "AUC: 0.9314\n",
      "F1 Score: 0.8350\n",
      "Precision: 0.8716\n",
      "Recall: 0.8012\n",
      "Specificity: 0.8797\n",
      "MCC: 0.6827\n",
      "AUPRC: 0.9285\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\User\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\torch\\nn\\modules\\transformer.py:379: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.batch_first was not True(use batch_first for better inference performance)\n",
      "  warnings.warn(\n",
      "C:\\Users\\User\\AppData\\Local\\Temp\\ipykernel_28104\\228421755.py:9: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model.load_state_dict(torch.load('SA_CA_model.pth'))\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAi8AAAHHCAYAAAB3K7g2AAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjkuMiwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8hTgPZAAAACXBIWXMAAA9hAAAPYQGoP6dpAAAoCUlEQVR4nO3de5hO9f7/8dc9p3vGnIU5iBmHiIhIvgwzbHIIOXxLssvMoCgkTKEShkxbziQdkeirIhW1I5KUcookYZxSziaDGWbGzPr94efe+zYzzDDjno+ej+tyXftea91rvdd9bfZzr3utGZtlWZYAAAAM4ebqAQAAAAqDeAEAAEYhXgAAgFGIFwAAYBTiBQAAGIV4AQAARiFeAACAUYgXAABgFOIFAAAYhXgBcEW7d+9Wq1atFBgYKJvNpiVLlhTp/vfv3y+bzaY5c+YU6X5N1qxZMzVr1szVYwAlFvECGGDPnj3q06ePKleuLG9vbwUEBCgqKkpTp07VuXPnivXYsbGx2rZtm1566SXNmzdPd999d7Ee70aKi4uTzWZTQEBAnp/j7t27ZbPZZLPZNGHChELv/9ChQxo1apS2bNlSBNMCuMTD1QMAuLJly5bpwQcflN1uV48ePVSrVi1lZmZq7dq1euaZZ7R9+3a98cYbxXLsc+fOad26dXr++efVv3//YjlGRESEzp07J09Pz2LZ/9V4eHgoPT1dn332mbp27eq0bv78+fL29tb58+evad+HDh3S6NGjFRkZqbp16xb4fcuXL7+m4wF/F8QLUILt27dP3bp1U0REhFatWqWwsDDHun79+ik5OVnLli0rtuMfP35ckhQUFFRsx7DZbPL29i62/V+N3W5XVFSU3n///VzxsmDBArVr106LFi26IbOkp6erVKlS8vLyuiHHA0zF10ZACTZ+/HidPXtWb7/9tlO4XFK1alUNHDjQ8frChQsaM2aMqlSpIrvdrsjISD333HPKyMhwel9kZKTat2+vtWvX6p577pG3t7cqV66sd99917HNqFGjFBERIUl65plnZLPZFBkZKeni1y2X/vN/GzVqlGw2m9OyFStWqEmTJgoKCpKfn5+qV6+u5557zrE+v3teVq1apaZNm8rX11dBQUHq2LGjduzYkefxkpOTFRcXp6CgIAUGBio+Pl7p6en5f7CX6d69u7744gudOnXKsWzDhg3avXu3unfvnmv7lJQUJSQkqHbt2vLz81NAQIDatm2rrVu3OrZZvXq1GjRoIEmKj493fP106TybNWumWrVqadOmTYqOjlapUqUcn8vl97zExsbK29s71/m3bt1awcHBOnToUIHPFbgZEC9ACfbZZ5+pcuXKaty4cYG27927t1588UXVq1dPkydPVkxMjJKSktStW7dc2yYnJ+uBBx7Qvffeq4kTJyo4OFhxcXHavn27JKlLly6aPHmyJOnhhx/WvHnzNGXKlELNv337drVv314ZGRlKTEzUxIkTdf/99+u777674vu++uortW7dWseOHdOoUaM0ePBgff/994qKitL+/ftzbd+1a1edOXNGSUlJ6tq1q+bMmaPRo0cXeM4uXbrIZrNp8eLFjmULFizQ7bffrnr16uXafu/evVqyZInat2+vSZMm6ZlnntG2bdsUExPjCIkaNWooMTFRkvT4449r3rx5mjdvnqKjox37OXnypNq2bau6detqypQpat68eZ7zTZ06VWXLllVsbKyys7MlSa+//rqWL1+u6dOnKzw8vMDnCtwULAAlUmpqqiXJ6tixY4G237JliyXJ6t27t9PyhIQES5K1atUqx7KIiAhLkrVmzRrHsmPHjll2u90aMmSIY9m+ffssSdYrr7zitM/Y2FgrIiIi1wwjR460/vuflcmTJ1uSrOPHj+c796VjzJ4927Gsbt26Vrly5ayTJ086lm3dutVyc3OzevToket4PXv2dNpn586drVtuuSXfY/73efj6+lqWZVkPPPCA1aJFC8uyLCs7O9sKDQ21Ro8enedncP78eSs7OzvXedjtdisxMdGxbMOGDbnO7ZKYmBhLkjVr1qw818XExDgt+/LLLy1J1tixY629e/dafn5+VqdOna56jsDNiCsvQAl1+vRpSZK/v3+Btv/8888lSYMHD3ZaPmTIEEnKdW9MzZo11bRpU8frsmXLqnr16tq7d+81z3y5S/fKfPLJJ8rJySnQew4fPqwtW7YoLi5OpUuXdiy/8847de+99zrO87/17dvX6XXTpk118uRJx2dYEN27d9fq1at15MgRrVq1SkeOHMnzKyPp4n0ybm4X//nMzs7WyZMnHV+Jbd68ucDHtNvtio+PL9C2rVq1Up8+fZSYmKguXbrI29tbr7/+eoGPBdxMiBeghAoICJAknTlzpkDbHzhwQG5ubqpatarT8tDQUAUFBenAgQNOyytWrJhrH8HBwfrrr7+uceLcHnroIUVFRal3794KCQlRt27d9MEHH1wxZC7NWb169VzratSooRMnTigtLc1p+eXnEhwcLEmFOpf77rtP/v7+WrhwoebPn68GDRrk+iwvycnJ0eTJk3XbbbfJbrerTJkyKlu2rH7++WelpqYW+Jjly5cv1M25EyZMUOnSpbVlyxZNmzZN5cqVK/B7gZsJ8QKUUAEBAQoPD9cvv/xSqPddfsNsftzd3fNcblnWNR/j0v0Yl/j4+GjNmjX66quv9Oijj+rnn3/WQw89pHvvvTfXttfjes7lErvdri5dumju3Ln6+OOP873qIknjxo3T4MGDFR0drffee09ffvmlVqxYoTvuuKPAV5iki59PYfz00086duyYJGnbtm2Fei9wMyFegBKsffv22rNnj9atW3fVbSMiIpSTk6Pdu3c7LT969KhOnTrleHKoKAQHBzs9mXPJ5Vd3JMnNzU0tWrTQpEmT9Ouvv+qll17SqlWr9PXXX+e570tz7ty5M9e63377TWXKlJGvr+/1nUA+unfvrp9++klnzpzJ8ybnSz766CM1b95cb7/9trp166ZWrVqpZcuWuT6TgoZkQaSlpSk+Pl41a9bU448/rvHjx2vDhg1Ftn/AJMQLUII9++yz8vX1Ve/evXX06NFc6/fs2aOpU6dKuvi1h6RcTwRNmjRJktSuXbsim6tKlSpKTU3Vzz//7Fh2+PBhffzxx07bpaSk5HrvpR/Wdvnj25eEhYWpbt26mjt3rlMM/PLLL1q+fLnjPItD8+bNNWbMGM2YMUOhoaH5bufu7p7rqs6HH36oP//802nZpcjKK/QKa+jQofr99981d+5cTZo0SZGRkYqNjc33cwRuZvyQOqAEq1KlihYsWKCHHnpINWrUcPoJu99//70+/PBDxcXFSZLq1Kmj2NhYvfHGGzp16pRiYmK0fv16zZ07V506dcr3Mdxr0a1bNw0dOlSdO3fWU089pfT0dL322muqVq2a0w2riYmJWrNmjdq1a6eIiAgdO3ZMM2fO1K233qomTZrku/9XXnlFbdu2VaNGjdSrVy+dO3dO06dPV2BgoEaNGlVk53E5Nzc3vfDCC1fdrn379kpMTFR8fLwaN26sbdu2af78+apcubLTdlWqVFFQUJBmzZolf39/+fr6qmHDhqpUqVKh5lq1apVmzpypkSNHOh7dnj17tpo1a6YRI0Zo/PjxhdofYDwXP+0EoAB27dplPfbYY1ZkZKTl5eVl+fv7W1FRUdb06dOt8+fPO7bLysqyRo8ebVWqVMny9PS0KlSoYA0fPtxpG8u6+Kh0u3btch3n8kd083tU2rIsa/ny5VatWrUsLy8vq3r16tZ7772X61HplStXWh07drTCw8MtLy8vKzw83Hr44YetXbt25TrG5Y8Tf/XVV1ZUVJTl4+NjBQQEWB06dLB+/fVXp20uHe/yR7Fnz55tSbL27duX72dqWc6PSucnv0elhwwZYoWFhVk+Pj5WVFSUtW7dujwfcf7kk0+smjVrWh4eHk7nGRMTY91xxx15HvO/93P69GkrIiLCqlevnpWVleW03aBBgyw3Nzdr3bp1VzwH4GZjs6xC3NEGAADgYtzzAgAAjEK8AAAAoxAvAADAKMQLAAAwCvECAACMQrwAAACjEC8AAMAoN+VP2PW5q7+rRwBQTP7aMMPVIwAoJt4FrBKuvAAAAKMQLwAAwCjECwAAMArxAgAAjEK8AAAAoxAvAADAKMQLAAAwCvECAACMQrwAAACjEC8AAMAoxAsAADAK8QIAAIxCvAAAAKMQLwAAwCjECwAAMArxAgAAjEK8AAAAoxAvAADAKMQLAAAwCvECAACMQrwAAACjEC8AAMAoxAsAADAK8QIAAIxCvAAAAKMQLwAAwCjECwAAMArxAgAAjEK8AAAAoxAvAADAKMQLAAAwCvECAACMQrwAAACjEC8AAMAoxAsAADAK8QIAAIxCvAAAAKMQLwAAwCjECwAAMArxAgAAjEK8AAAAoxAvAADAKMQLAAAwCvECAACMQrwAAACjEC8AAMAoxAsAADAK8QIAAIxCvAAAAKMQLwAAwCjECwAAMArxAgAAjEK8AAAAoxAvAADAKMQLAAAwCvECAACMQrwAAACjEC8AAMAoxAsAADAK8QIAAIxCvAAAAKMQLwAAwCjECwAAMArxAgAAjEK8AAAAoxAvAADAKMQLAAAwCvECAACMQrwAAACjEC8AAMAoxAsAADAK8QIAAIxCvAAAAKMQLwAAwCjECwAAMArxAgAAjEK8AAAAoxAvAADAKMQLAAAwCvECAACMQrwAAACjEC8AAMAoxAsAADAK8QIAAIxCvAAAAKMQLwAAwCjEC0qkqHpV9NGUPtq7/CWd+2mGOjS702n9833u05bFL+jE9xN16JvxWjarvxrUinDapu7tt2rpa/11eM14/fH1vzTjhYfl6+N1I08DQAFs2rhBA57sq5bNmqjOHdW1auVXTutPnjihEc8NU8tmTdSwfh098XgvHTiw3zXDokQgXlAi+frYtW3Xn3o6aWGe65MPHNOgf32oux8cpxbxk3TgUIo+m9lfZYL9JElhZQO1bNYA7Tl4XNGPTlDHfq+qZpVQvZn46I08DQAFcO5cuqpXr67hL4zMtc6yLD39VD/98cdBTZk+Uws/+lhh4eXVp1e80tPTXTAtSgIPVw8A5GX5d79q+Xe/5rt+4b83Or0eOnGx4js3Vq3bwrV6/S61bVpLWRey9XTSB7IsS5I04KWF2vjhc6pcoYz2HjxRrPMDKLgmTWPUpGlMnusOHNivn7du0aJPlqpq1dskSS+8OEr/iInSvz9fpi4PPHgjR0UJ4dJ4OXHihN555x2tW7dOR44ckSSFhoaqcePGiouLU9myZV05Hgzh6eGuXl2idOpMurbt+lOSZPfyUFZWtiNcJOlcRqYkqXHdKsQLYIiszIt/b+1edscyNzc3eXl56afNm4iXvymXfW20YcMGVatWTdOmTVNgYKCio6MVHR2twMBATZs2Tbfffrs2btx49R3hb6tt01o6/t1EnfpxsgY80lzt+87QyVNpkqTV63cq5JYADerRQp4e7gry99HYpzpKkkLLBrpybACFEFmpssLCwjVtykSdTk1VVmam3nnrDR09ckTHjx939XhwEZddeRkwYIAefPBBzZo1SzabzWmdZVnq27evBgwYoHXr1l1xPxkZGcrIyHB+f062bG7uRT4zSpZvNuxSw25JKhPkp/gujfXe+J6KfnSCjv91Vjv2HtFjL87Ty0O6KHHA/crOydHM97/RkROnZeXkuHp0AAXk6empSVOna9SI59W08T1yd3dXw/9ppCZNo52urOLvxWXxsnXrVs2ZMydXuEiSzWbToEGDdNddd111P0lJSRo9erTTMveQBvIMu6fIZkXJlH4+U3sPntDegye0ftt+bfvkRcV2bqwJ7yyXdPG+mIX/3qhypf2Vdi5DliU99cg/tO+Pky6eHEBh1Lyjlj5Y/InOnDmjrKwslS5dWv/s9qDuuKOWq0eDi7jsa6PQ0FCtX78+3/Xr169XSEjIVfczfPhwpaamOv3xCKlflKPCEG42m+yeuXv8WMoZpZ3L1AOt6+l8ZpZW/vCbC6YDcL38/f1VunRpHTiwX79u/0XN/tHC1SPBRVx25SUhIUGPP/64Nm3apBYtWjhC5ejRo1q5cqXefPNNTZgw4ar7sdvtstvtTsv4ysh8vj5eqlLhPzdsR5a/RXdWK6+/Tqfr5Kk0De3dWsu+2aYjJ1J1S5Cf+nSNVni5IC1esdnxnr4PReuHrXt1Nj1TLf7ndo17upNGTP9EqWfPueKUAOQjPS1Nv//+u+P1n3/8od927FBgYKDCwsO1/MsvFBxcWmFh4dq9e6fGJ41T83+0VOOoJi6cGq5ks1z4peHChQs1efJkbdq0SdnZ2ZIkd3d31a9fX4MHD1bXrl2vab8+d/UvyjHhAk3r36blbw3MtXzepz9owEv/p7nj4tSgdqRuCfJVSmq6Nm4/oH+9+W9t+vU//wC+NeZRtWlSS36lvLRz/1FNeXel3l+24UaeBorBXxtmuHoEFLEN639U7/geuZbf37Gzxox7WfPfe1dzZ7+tkydOqmzZsmp/f0f16fukPL34oZM3G+8CXlJxabxckpWVpRMnLj66WqZMGXl6el7X/ogX4OZFvAA3r4LGS4n4IXWenp4KCwtz9RgAAMAA/HoAAABgFOIFAAAYhXgBAABGIV4AAIBRiBcAAGAU4gUAABiFeAEAAEYhXgAAgFGIFwAAYBTiBQAAGIV4AQAARiFeAACAUYgXAABgFOIFAAAYhXgBAABGIV4AAIBRiBcAAGAU4gUAABiFeAEAAEYhXgAAgFGIFwAAYBTiBQAAGIV4AQAARiFeAACAUYgXAABgFOIFAAAYhXgBAABGIV4AAIBRiBcAAGAU4gUAABiFeAEAAEYhXgAAgFGIFwAAYBTiBQAAGIV4AQAARiFeAACAUYgXAABgFOIFAAAYhXgBAABGIV4AAIBRiBcAAGAU4gUAABiFeAEAAEYhXgAAgFGIFwAAYBTiBQAAGIV4AQAARiFeAACAUYgXAABgFOIFAAAYhXgBAABGIV4AAIBRiBcAAGAU4gUAABiFeAEAAEYhXgAAgFGIFwAAYBTiBQAAGIV4AQAARiFeAACAUYgXAABgFOIFAAAYhXgBAABGIV4AAIBRiBcAAGAU4gUAABiFeAEAAEYhXgAAgFGIFwAAYBTiBQAAGIV4AQAARiFeAACAUYgXAABglGuKl2+//VaPPPKIGjVqpD///FOSNG/ePK1du7ZIhwMAALhcoeNl0aJFat26tXx8fPTTTz8pIyNDkpSamqpx48YV+YAAAAD/rdDxMnbsWM2aNUtvvvmmPD09HcujoqK0efPmIh0OAADgcoWOl507dyo6OjrX8sDAQJ06daooZgIAAMhXoeMlNDRUycnJuZavXbtWlStXLpKhAAAA8lPoeHnsscc0cOBA/fjjj7LZbDp06JDmz5+vhIQEPfHEE8UxIwAAgINHYd8wbNgw5eTkqEWLFkpPT1d0dLTsdrsSEhI0YMCA4pgRAADAwWZZlnUtb8zMzFRycrLOnj2rmjVrys/Pr6hnu2Y+d/V39QgAislfG2a4egQAxcS7gJdUCn3l5RIvLy/VrFnzWt8OAABwTQodL82bN5fNZst3/apVq65rIAAAgCspdLzUrVvX6XVWVpa2bNmiX375RbGxsUU1FwAAQJ4KHS+TJ0/Oc/moUaN09uzZ6x4IAADgSorsFzM+8sgjeuedd4pqdwAAAHm65ht2L7du3Tp5e3sX1e6uy66VE109AoBiEtxugqtHAFBMzn2ZUKDtCh0vXbp0cXptWZYOHz6sjRs3asSIEYXdHQAAQKEUOl4CAwOdXru5ual69epKTExUq1atimwwAACAvBQqXrKzsxUfH6/atWsrODi4uGYCAADIV6Fu2HV3d1erVq347dEAAMBlCv20Ua1atbR3797imAUAAOCqCh0vY8eOVUJCgpYuXarDhw/r9OnTTn8AAACKU4HveUlMTNSQIUN03333SZLuv/9+p18TYFmWbDabsrOzi35KAACA/6/Av1Xa3d1dhw8f1o4dO664XUxMTJEMdj0OpmS4egQAxaTaw9NdPQKAYlLkP+flUuOUhDgBAAB/X4W65+VKv00aAADgRijUz3mpVq3aVQMmJSXlugYCAAC4kkLFy+jRo3P9hF0AAIAbqVDx0q1bN5UrV664ZgEAALiqAt/zwv0uAACgJChwvBTwiWoAAIBiVeCvjXJycopzDgAAgAIp9K8HAAAAcCXiBQAAGIV4AQAARiFeAACAUYgXAABgFOIFAAAYhXgBAABGIV4AAIBRiBcAAGAU4gUAABiFeAEAAEYhXgAAgFGIFwAAYBTiBQAAGIV4AQAARiFeAACAUYgXAABgFOIFAAAYhXgBAABGIV4AAIBRiBcAAGAU4gUAABiFeAEAAEYhXgAAgFGIFwAAYBTiBQAAGIV4AQAARiFeAACAUYgXAABgFOIFAAAYhXgBAABGIV4AAIBRiBcAAGAU4gUAABiFeAEAAEYhXgAAgFGIFwAAYBTiBQAAGIV4AQAARiFeAACAUYgXAABgFOIFAAAYhXgBAABGIV4AAIBRiBcAAGAU4gUAABiFeAEAAEYhXgAAgFGIFwAAYBTiBQAAGIV4AQAARiFeAACAUYgXAABgFOIFAAAYhXgBAABGIV4AAIBRiBcAAGAU4gUAABiFeAEAAEYhXgAAgFGIFwAAYBTiBQAAGIV4AQAARiFeAACAUYgXAABgFOIFAAAYhXgBAABGIV4AAIBRiBcAAGAU4gUAABjFw9UDAAXx6eKF+mzxBzp6+JAkKaJyFT3as4/uadRUp1NTNfetmdq0/nsdO3JEgcHBior+h+Ie7yc/P38XTw7gclG1btWgBxuo3m0hCrvFT11HLdFn65IlSR7ubhoV10StG1RSpbAgnU7L0KqfDmjE22t0OCXNsY+6VctpbK9o1a8WquwcS0vW7tLQ11cr7XyWq04LNxBXXmCEsmVD1PvJpzVzzv9p5uz3dVf9e/TiswO1f2+yTp44ppMnjqlP/yF6a/5iPfvCGG344TtNHDfS1WMDyIOvt6e27T2mp2d8lWtdKbuH6lYtp5cX/KBG/d5Vt8RPVO3W0vpwdGfHNmGlfbXs5Qe159ApRQ+cr47PL1LNiDJ6M6HtjTwNuBBXXmCERk2bOb3u2fcpfbb4A+345We1vb+LRiVNdqwLv7WCevYZoJdHD1f2hQty9+C/5kBJsnzjPi3fuC/PdafTM9V++EdOywa9ulJrpz+iCmX9dfD4GbVtWEVZF3L09IyvZFkXtxkwbYU2vh6nyuFB2nvoVDGfAVyNKy8wTnZ2tr5e8YXOnz+nmrXr5LlNWtoZlfL1I1yAm0CAr5dyciydSsuQJNk93ZV1IdsRLpJ0LvOCJKnxHeVdMSJusBIdLwcPHlTPnj2vuE1GRoZOnz7t9CcjI+MGTYgbaW/yLrX/R0O1jblbU8aP1aiXpyiiUpVc26We+kvvzX5D7Tr+rwumBFCU7J7uGtsrWh+s3qEz6ZmSpNVbf1dIsK8GPdBAnh5uCvKza2zPaElSaGlfV46LG6REx0tKSormzp17xW2SkpIUGBjo9OfVKeNv0IS4kSpEVNLrcz/UjLfmq0Pnrho/5gUd2LfHaZu0tLN6fkg/RURWVo/eT7hoUgBFwcPdTe8930E22fTU9P/cH7PjwEk9NuELPfW/dyvl06e1//0ntP9Iqo6kpDldjcHNy6XX1D/99NMrrt+7d+9V9zF8+HANHjzYadmxtHw2htE8PT1VvkJFSVK122tq545ftHjhfA0a9qIkKT0tTcOffkI+pXw1+uUp8vDwdOW4AK6Dh7ub5j/fQRVDAtT22Q8cV10uWfj1b1r49W8qF1RKaeezZFnSU13qa9/hU64ZGDeUS+OlU6dOstlssq6Qyjab7Yr7sNvtstvtTstSL/C10d+BZeUoK+viP2hpaWc17Om+8vT00phXpsnrsv9OADDHpXCpUj5YbZ5dqJQz5/Pd9tipdElSj1a1dD4rWys3H7hRY8KFXPq1UVhYmBYvXqycnJw8/2zevNmV46EEeWvmVP3800YdOfyn9ibv0lszp2rr5o1q0bqd0tLOaujAPjp/7pwSnhut9LQ0pZw8oZSTJ5Sdne3q0QFcxtfbU3dWLqs7K5eVJEWGBurOymVVoay/PNzdtGDE/apXLUTx/1omdzebQoJLKSS4lDw9/vM/WX3vv0t1q5ZT1fLB6tOhrib3a6EX31mj1DT+z+vfgUuvvNSvX1+bNm1Sx44d81x/tasy+Ps49VeK/pX4glJOHpevn58qVamml6fMUv17GmnL5g36bfs2SVKPB9s5ve+9xV8oNIynD4CSpF61UC1/5SHH6/F9m0uS5i3/RWPf+14dGlWVJK1/Ldbpfa2eWahvfz4oSbq7eqheeLSx/Lw9tfOPFPWftkLvr/z1Bp0BXM1mubAOvv32W6WlpalNmzZ5rk9LS9PGjRsVExNTqP0eTKG8gZtVtYenu3oEAMXk3JcJBdrOpVdemjZtesX1vr6+hQ4XAABwcyvRj0oDAABcjngBAABGIV4AAIBRiBcAAGAU4gUAABiFeAEAAEYhXgAAgFGIFwAAYBTiBQAAGIV4AQAARiFeAACAUYgXAABgFOIFAAAYhXgBAABGIV4AAIBRiBcAAGAU4gUAABiFeAEAAEYhXgAAgFGIFwAAYBTiBQAAGIV4AQAARiFeAACAUYgXAABgFOIFAAAYhXgBAABGIV4AAIBRiBcAAGAU4gUAABiFeAEAAEYhXgAAgFGIFwAAYBTiBQAAGIV4AQAARiFeAACAUYgXAABgFOIFAAAYhXgBAABGIV4AAIBRiBcAAGAU4gUAABiFeAEAAEYhXgAAgFGIFwAAYBTiBQAAGIV4AQAARiFeAACAUYgXAABgFOIFAAAYhXgBAABGIV4AAIBRiBcAAGAU4gUAABiFeAEAAEYhXgAAgFGIFwAAYBTiBQAAGIV4AQAARiFeAACAUYgXAABgFOIFAAAYhXgBAABGIV4AAIBRiBcAAGAU4gUAABiFeAEAAEYhXgAAgFGIFwAAYBTiBQAAGIV4AQAARiFeAACAUYgXAABgFOIFAAAYhXgBAABGIV4AAIBRiBcAAGAU4gUAABiFeAEAAEYhXgAAgFGIFwAAYBTiBQAAGIV4AQAARiFeAACAUYgXAABgFOIFAAAYhXgBAABGsVmWZbl6COBaZWRkKCkpScOHD5fdbnf1OACKEH+/kR/iBUY7ffq0AgMDlZqaqoCAAFePA6AI8fcb+eFrIwAAYBTiBQAAGIV4AQAARiFeYDS73a6RI0dyMx9wE+LvN/LDDbsAAMAoXHkBAABGIV4AAIBRiBcAAGAU4gUAABiFeIHRXn31VUVGRsrb21sNGzbU+vXrXT0SgOu0Zs0adejQQeHh4bLZbFqyZImrR0IJQ7zAWAsXLtTgwYM1cuRIbd68WXXq1FHr1q117NgxV48G4DqkpaWpTp06evXVV109CkooHpWGsRo2bKgGDRpoxowZkqScnBxVqFBBAwYM0LBhw1w8HYCiYLPZ9PHHH6tTp06uHgUlCFdeYKTMzExt2rRJLVu2dCxzc3NTy5YttW7dOhdOBgAobsQLjHTixAllZ2crJCTEaXlISIiOHDnioqkAADcC8QIAAIxCvMBIZcqUkbu7u44ePeq0/OjRowoNDXXRVACAG4F4gZG8vLxUv359rVy50rEsJydHK1euVKNGjVw4GQCguHm4egDgWg0ePFixsbG6++67dc8992jKlClKS0tTfHy8q0cDcB3Onj2r5ORkx+t9+/Zpy5YtKl26tCpWrOjCyVBS8Kg0jDZjxgy98sorOnLkiOrWratp06apYcOGrh4LwHVYvXq1mjdvnmt5bGys5syZc+MHQolDvAAAAKNwzwsAADAK8QIAAIxCvAAAAKMQLwAAwCjECwAAMArxAgAAjEK8AAAAoxAvAEqsuLg4derUyfG6WbNmevrpp2/4HKtXr5bNZtOpU6du+LEB5Ea8ACi0uLg42Ww22Ww2eXl5qWrVqkpMTNSFCxeK9biLFy/WmDFjCrQtwQHcvPjdRgCuSZs2bTR79mxlZGTo888/V79+/eTp6anhw4c7bZeZmSkvL68iOWbp0qWLZD8AzMaVFwDXxG63KzQ0VBEREXriiSfUsmVLffrpp46vel566SWFh4erevXqkqSDBw+qa9euCgoKUunSpdWxY0ft37/fsb/s7GwNHjxYQUFBuuWWW/Tss8/q8t9ecvnXRhkZGRo6dKgqVKggu92uqlWr6u2339b+/fsdvxsnODhYNptNcXFxki7+9vGkpCRVqlRJPj4+qlOnjj766COn43z++eeqVq2afHx81Lx5c6c5Abge8QKgSPj4+CgzM1OStHLlSu3cuVMrVqzQ0qVLlZWVpdatW8vf31/ffvutvvvuO/n5+alNmzaO90ycOFFz5szRO++8o7Vr1yolJUUff/zxFY/Zo0cPvf/++5o2bZp27Nih119/XX5+fqpQoYIWLVokSdq5c6cOHz6sqVOnSpKSkpL07rvvatasWdq+fbsGDRqkRx55RN98842ki5HVpUsXdejQQVu2bFHv3r01bNiw4vrYAFwLCwAKKTY21urYsaNlWZaVk5NjrVixwrLb7VZCQoIVGxtrhYSEWBkZGY7t582bZ1WvXt3KyclxLMvIyLB8fHysL7/80rIsywoLC7PGjx/vWJ+VlWXdeuutjuNYlmXFxMRYAwcOtCzLsnbu3GlJslasWJHnjF9//bUlyfrrr78cy86fP2+VKlXK+v7775227dWrl/Xwww9blmVZw4cPt2rWrOm0fujQobn2BcB1uOcFwDVZunSp/Pz8lJWVpZycHHXv3l2jRo1Sv379VLt2baf7XLZu3ark5GT5+/s77eP8+fPas2ePUlNTdfjwYTVs2NCxzsPDQ3fffXeur44u2bJli9zd3RUTE1PgmZOTk5Wenq57773XaXlmZqbuuusuSdKOHTuc5pCkRo0aFfgYAIof8QLgmjRv3lyvvfaavLy8FB4eLg+P//xz4uvr67Tt2bNnVb9+fc2fPz/XfsqWLXtNx/fx8Sn0e86ePStJWrZsmcqXL++0zm63X9McAG484gXANfH19VXVqlULtG29evW0cOFClStXTgEBAXluExYWph9//FHR0dGSpAsXLmjTpk2qV69entvXrl1bOTk5+uabb9SyZctc6y9d+cnOznYsq1mzpux2u37//fd8r9jUqFFDn376qdOyH3744eonCeCG4YZdAMXun//8p8qUKaOOHTvq22+/1b59+7R69Wo99dRT+uOPPyRJAwcO1Msvv6wlS5bot99+05NPPnnFn9ESGRmp2NhY9ezZU0uWLHHs84MPPpAkRUREyGazaenSpTp+/LjOnj0rf39/JSQkaNCgQZo7d6727NmjzZs3a/r06Zo7d64kqW/fvtq9e7eeeeYZ7dy5UwsWLNCcOXOK+yMCUAjEC4BiV6pUKa1Zs0YVK1ZUly5dVKNGDfXq1Uvnz593XIkZMmSIHn30UcXGxqpRo0by9/dX586dr7jf1157TQ888ICefPJJ3X777XrssceUlpYmSSpfvrxGjx6tYcOGKSQkRP3795ckjRkzRiNGjFBSUpJq1KihNm3aaNmyZapUqZIkqWLFilq0aJGWLFmiOnXqaNasWRo3blwxfjoACstm5Xc3HAAAQAnElRcAAGAU4gUAABiFeAEAAEYhXgAAgFGIFwAAYBTiBQAAGIV4AQAARiFeAACAUYgXAABgFOIFAAAYhXgBAABGIV4AAIBR/h9c9Fxuk8xffwAAAABJRU5ErkJggg==",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# save the model\n",
    "torch.save(model.state_dict(), 'SA_CA_model.pth')\n",
    "\n",
    "# load data\n",
    "_,_,_,_,train_loader, test_loader = get_train_test_loader()\n",
    "\n",
    "# load the model\n",
    "model = main_training(train=False)\n",
    "model.load_state_dict(torch.load('SA_CA_model.pth'))\n",
    "\n",
    "# Evaluate the loaded model\n",
    "metrics = test_model(model, test_loader)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
